---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Interrogating Linear Regression with Non-Linear Proxies

Researchers often treat monotonic proxies as “close enough” to the causal variable. This walkthrough recreates a stress–anxiety scenario to show how that shortcut breaks regression interpretability. Each question below matches the challenge rubric, pairing code-generated evidence with plain-language interpretation.

## Data and Proxy Diagnostics

```{python}
#| label: setup
#| include: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy import stats

observDF = pd.DataFrame(
    {
        "Stress": [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
        "StressSurvey": [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
        "Time": [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
        "Anxiety": [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22],
    }
)

sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (8, 5)


def fit_ols(features, data):
    X = sm.add_constant(data[features])
    y = data["Anxiety"]
    return sm.OLS(y, X).fit()


def regression_markdown(model):
    params = model.params.rename({"const": "Intercept"})
    conf_int = model.conf_int().rename(index={"const": "Intercept"})
    stats_df = pd.DataFrame(
        {
            "Estimate": params.round(3),
            "Std. Error": model.bse.round(3),
            "t": model.tvalues.round(2),
            "p-value": model.pvalues.apply(lambda x: f"{x:.3g}"),
            "95% CI Low": conf_int[0].round(3),
            "95% CI High": conf_int[1].round(3),
        }
    )
    return stats_df.to_markdown()


def get_value(series, name):
    if name in series.index:
        return series[name]
    if name == "Intercept" and "const" in series.index:
        return series["const"]
    raise KeyError(f"{name} not found in index {list(series.index)}")


model_results = {
    "q1_stresssurvey": fit_ols(["StressSurvey"], observDF),
    "q3_time": fit_ols(["Time"], observDF),
    "q5_proxy_multi": fit_ols(["StressSurvey", "Time"], observDF),
    "q6_true_multi": fit_ols(["Stress", "Time"], observDF),
}

subset_low = observDF[observDF["StressSurvey"] <= 6].copy()
model_results["q9_subset"] = fit_ols(["StressSurvey", "Time"], subset_low)

model1 = model_results["q1_stresssurvey"]
model3 = model_results["q3_time"]
model5 = model_results["q5_proxy_multi"]
model6 = model_results["q6_true_multi"]
model9 = model_results["q9_subset"]
```

```{python}
#| label: tbl-observations
#| tbl-cap: "Observed data with known true relationships"
observDF
```

```{python}
#| label: fig-stress-proxy
#| fig-cap: "StressSurvey is monotonic but not linear in true stress"
fig, ax = plt.subplots()
ax.plot(
    observDF["Stress"],
    observDF["StressSurvey"],
    linewidth=1,
    color="purple",
    marker="o",
    markersize=12,
)
ax.set_title("Survey Proxy vs. Actual Stress")
ax.set_xlabel("Actual Stress Level")
ax.set_ylabel("Stress Survey Response")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## Analysis and Answers

### Question 1: Bivariate Regression with StressSurvey

```{python}
#| label: reg1-stresssurvey
print("=" * 70)
print("Question 1: Bivariate Regression - Anxiety ~ StressSurvey")
print("=" * 70)
print(regression_markdown(model1))
print(f"\nR-squared: {model1.rsquared:.3f}")
```

The estimated regression line is

$$\widehat{Anxiety} = -1.524 + 1.047 \times StressSurvey.$$

For beginners, read this as: start at −1.524 when the survey reports zero stress, and then add about 1.05 anxiety points for every one-unit bump on the survey scale. The healthy part of the story is the $R^2$ of 0.901, which says the line hugs the data closely. The worrying part is that the survey tries to stand in for real cortisol, and it stretches more aggressively at higher stress levels. The regression compensates by pushing the slope above 1 and by dropping the intercept below zero. So the equation “fits” but quietly distorts the true cause-and-effect relationship.

### Question 2: Visualization of Bivariate Relationship – StressSurvey

```{python}
#| label: fig-stresssurvey-anxiety
#| fig-cap: "StressSurvey vs Anxiety with fitted line"
fig, ax = plt.subplots()
ax.scatter(
    observDF["StressSurvey"],
    observDF["Anxiety"],
    s=100,
    alpha=0.7,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
)
x_line = np.linspace(observDF["StressSurvey"].min(), observDF["StressSurvey"].max(), 100)
x_line_df = pd.DataFrame({"StressSurvey": x_line})
y_line = model1.predict(sm.add_constant(x_line_df))
ax.plot(x_line, y_line, "r-", linewidth=2, label=f"Regression line (R² = {model1.rsquared:.3f})")
ax.set_xlabel("Stress Survey Response", fontsize=12, fontweight="bold")
ax.set_ylabel("Anxiety Level", fontsize=12, fontweight="bold")
ax.set_title("Bivariate Relationship: StressSurvey vs Anxiety", fontsize=14, fontweight="bold")
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

The dots show Anxiety (vertical axis) for each survey response (horizontal axis). Notice the horizontal “steps”: the survey records three different anxiety outcomes for every real stress level. The final step from 9 to 12 shoots upward more sharply than earlier steps. The red line averages across those steps, so it floats above the low-stress points and sinks below the high-stress ones. This picture is the geometric version of the warning above—our proxy is monotonic, yet the violation of straight-line growth silently bends the fitted slope.

### Question 3: Bivariate Regression with Time

```{python}
#| label: reg3-time
print("=" * 70)
print("Question 3: Bivariate Regression - Anxiety ~ Time")
print("=" * 70)
print(regression_markdown(model3))
print(f"\nR-squared: {model3.rsquared:.3f}")
```

Regressing Anxiety on Time produces

$$\widehat{Anxiety} = -3.680 + 5.341 \times Time,$$

with $R^2 = 0.563$. Interpreted literally, the model says “start at −3.68 anxiety points and add 5.34 points for each extra hour online.” That story is implausible (negative anxiety at zero time!) and it clashes with the true 0.1 coefficient we built into the data. The culprit is omitted-variable bias: Stress is missing from this regression, so the model stuffs stress-driven variation into the Time coefficient and inflates it more than fifty-fold.

### Question 4: Visualization of Bivariate Relationship – Time

```{python}
#| label: fig-time-anxiety
#| fig-cap: "Time vs Anxiety with fitted line"
fig, ax = plt.subplots()
ax.scatter(
    observDF["Time"],
    observDF["Anxiety"],
    s=100,
    alpha=0.7,
    color="coral",
    edgecolors="black",
    linewidth=1.5,
)
x_line = np.linspace(observDF["Time"].min(), observDF["Time"].max(), 100)
x_line_df = pd.DataFrame({"Time": x_line})
y_line = model3.predict(sm.add_constant(x_line_df))
ax.plot(x_line, y_line, "r-", linewidth=2, label=f"Regression line (R² = {model3.rsquared:.3f})")
ax.set_xlabel("Time on Social Media (hours)", fontsize=12, fontweight="bold")
ax.set_ylabel("Anxiety Level", fontsize=12, fontweight="bold")
ax.set_title("Bivariate Relationship: Time vs Anxiety", fontsize=14, fontweight="bold")
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

Each dot still plots Time (horizontal) versus Anxiety (vertical), but the dots now form tall columns instead of a tidy upward march. That “vertical” pattern means anxiety differs wildly even when Time stays fixed—because those students are sitting at different stress levels. The regression line, oblivious to stress, leans steeply upward and claims social media is the hero or villain. Visually, you can see why the coefficient exaggerates the effect: the model chases vertical variation it cannot explain.

### Question 5: Multiple Regression with StressSurvey and Time

```{python}
#| label: reg5-multiple-stresssurvey
print("=" * 70)
print("Question 5: Multiple Regression - Anxiety ~ StressSurvey + Time")
print("=" * 70)
print(regression_markdown(model5))
print(f"\nR-squared: {model5.rsquared:.3f}")
```

The fitted model is

$$\widehat{Anxiety} = 0.589 + 1.427 \times StressSurvey - 2.780 \times Time,$$

with $R^2 = 0.935$. If you read the output like a beginner, you might conclude: “Every extra survey point increases anxiety by 1.43, but every extra hour online *reduces* anxiety by 2.78.” Both coefficients look statistically convincing, yet both are wrong. The survey proxy is still curving away from the true stress measure, so the regression leans harder on StressSurvey and flips the Time sign to balance things out. We now have a high-$R^2$ model confidently proclaiming the opposite of the truth.

### Question 6: Multiple Regression with True Stress and Time

```{python}
#| label: reg6-multiple-stress
print("=" * 70)
print("Question 6: Multiple Regression - Anxiety ~ Stress + Time")
print("=" * 70)
print(regression_markdown(model6))
print(f"\nR-squared: {model6.rsquared:.3f}")
```

Substituting the actual stress measure lands exactly on the data-generating process:

$$\widehat{Anxiety} = 0.000 + 1.000 \times Stress + 0.100 \times Time,$$

with $R^2 = 1.000$. Now the intercept is exactly zero (no stress, no time, no anxiety), each stress point adds one full point of anxiety, and each hour online adds the modest 0.1 bump we engineered. Perfect $R^2$ simply means the line passes through every observed point. Nothing about the sample changed—only the quality of the stress measurement. That small swap instantly restores trustworthy coefficients and a sensible story.

### Question 7: Model Comparison

```{python}
#| label: reg7-comparison
print("=" * 70)
print("Question 7: Model Comparison")
print("=" * 70)

print("\nModel 1: Anxiety ~ StressSurvey + Time")
print("-" * 70)
print(f"  R-squared: {model5.rsquared:.4f}")
print(f"  Intercept: {get_value(model5.params, 'Intercept'):.4f}")
print(f"  StressSurvey coefficient: {get_value(model5.params, 'StressSurvey'):.4f}")
print(f"  Time coefficient: {get_value(model5.params, 'Time'):.4f}")
print(f"\n  Statistical Significance:")
pvals5 = model5.pvalues
print(f"    Intercept p-value: {get_value(pvals5, 'Intercept'):.4e}")
print(f"    StressSurvey p-value: {get_value(pvals5, 'StressSurvey'):.4e}")
print(f"    Time p-value: {get_value(pvals5, 'Time'):.4e}")

print("\nModel 2: Anxiety ~ Stress + Time")
print("-" * 70)
print(f"  R-squared: {model6.rsquared:.4f}")
print(f"  Intercept: {get_value(model6.params, 'Intercept'):.4f}")
print(f"  Stress coefficient: {get_value(model6.params, 'Stress'):.4f}")
print(f"  Time coefficient: {get_value(model6.params, 'Time'):.4f}")
print(f"\n  Statistical Significance:")
pvals6 = model6.pvalues
print(f"    Intercept p-value: {get_value(pvals6, 'Intercept'):.4e}")
print(f"    Stress p-value: {get_value(pvals6, 'Stress'):.4e}")
print(f"    Time p-value: {get_value(pvals6, 'Time'):.4e}")

print("\n" + "=" * 70)
print("Key Comparison Points:")
print("=" * 70)

stresssurvey_pct_error = (get_value(model5.params, "StressSurvey") - 1.0) / 1.0 * 100
stress_pct_error = (get_value(model6.params, "Stress") - 1.0) / 1.0 * 100
time_proxy_pct_error = (get_value(model5.params, "Time") - 0.1) / 0.1 * 100
time_true_pct_error = (get_value(model6.params, "Time") - 0.1) / 0.1 * 100

print(f"\n1. R-squared:")
print(f"   Proxy model: {model5.rsquared:.4f}")
print(f"   True-stress model: {model6.rsquared:.4f}")
print(f"   Absolute difference: {abs(model5.rsquared - model6.rsquared):.4f}")

print(f"\n2. Stress-related coefficient:")
print(f"   Proxy coefficient: {get_value(model5.params, 'StressSurvey'):.4f} ({stresssurvey_pct_error:.1f}% off)")
print(f"   True coefficient: {get_value(model6.params, 'Stress'):.4f} ({stress_pct_error:.1f}% off)")

print(f"\n3. Time coefficient:")
print(f"   Proxy model: {get_value(model5.params, 'Time'):.4f} ({time_proxy_pct_error:.1f}% off)")
print(f"   True-stress model: {get_value(model6.params, 'Time'):.4f} ({time_true_pct_error:.1f}% off)")

print("\n4. Statistical significance:")
print("   Both models clear the 5% threshold on every coefficient, despite only one set matching reality.")
```

```{python}
#| label: fig-comparison
#| fig-cap: "Coefficient comparison between proxy-based and true-stress models"
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax1 = axes[0]
coefficients = ["StressSurvey\n(Proxy Model)", "Stress\n(True Model)", "True Value"]
values = [get_value(model5.params, "StressSurvey"), get_value(model6.params, "Stress"), 1.0]
colors = ["steelblue", "coral", "green"]
bars = ax1.bar(coefficients, values, color=colors, alpha=0.7, edgecolor="black", linewidth=1.5)
ax1.axhline(y=1.0, color="green", linestyle="--", linewidth=2, label="True Value (1.0)")
ax1.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax1.set_title("Stress Coefficient Comparison", fontsize=12, fontweight="bold")
ax1.legend()
ax1.grid(True, alpha=0.3, axis="y")
for bar, val in zip(bars, values):
    ax1.text(bar.get_x() + bar.get_width() / 2.0, val, f"{val:.3f}", ha="center", va="bottom", fontweight="bold")

ax2 = axes[1]
coefficients_time = ["Proxy Model", "True Model", "True Value"]
values_time = [get_value(model5.params, "Time"), get_value(model6.params, "Time"), 0.1]
bars2 = ax2.bar(coefficients_time, values_time, color=colors, alpha=0.7, edgecolor="black", linewidth=1.5)
ax2.axhline(y=0.1, color="green", linestyle="--", linewidth=2, label="True Value (0.1)")
ax2.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax2.set_title("Time Coefficient Comparison", fontsize=12, fontweight="bold")
ax2.legend()
ax2.grid(True, alpha=0.3, axis="y")
for bar, val in zip(bars2, values_time):
    ax2.text(bar.get_x() + bar.get_width() / 2.0, val, f"{val:.3f}", ha="center", va="bottom", fontweight="bold")

plt.tight_layout()
plt.show()
```

To read the comparison, first notice that each model produces a headline number for fit ($R^2$) and individual coefficients for Stress and Time. The charts echo the tables: bars closer to the green dashed lines tell the truth; bars that wander away are misleading.

*R-squared:* Think of $R^2$ as the share of wiggle in Anxiety the model explains. The proxy model posts 0.935—almost indistinguishable from the perfect 1.000 of the true-stress model. If you judged models by this statistic alone, you would celebrate both.

*Coefficients:* The real action lives here. The proxy model claims each survey point adds 1.427 anxiety points (43% too high) and each extra hour online *reduces* anxiety by 2.780. The true-stress model lands exactly on the known values of 1.000 and +0.100. In other words, small coefficient errors in the bar chart translate into entirely different policy stories.

*Significance:* Both models report tiny p-values for every term. Statistical significance just says “this number is unlikely to be zero if the model were correct.” It does not guarantee the number is the *right* causal effect. That is why researchers must go beyond the asterisk column in regression tables.

```{python}
#| label: tbl-model-summary
summary_rows = []
summary_rows.append(
    {
        "Model": "Anxiety ~ StressSurvey",
        "Intercept": get_value(model1.params, "Intercept"),
        "Stress/Proxy": get_value(model1.params, "StressSurvey"),
        "Time": "",
        "R-squared": model1.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ Time",
        "Intercept": get_value(model3.params, "Intercept"),
        "Stress/Proxy": "",
        "Time": get_value(model3.params, "Time"),
        "R-squared": model3.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ StressSurvey + Time",
        "Intercept": get_value(model5.params, "Intercept"),
        "Stress/Proxy": get_value(model5.params, "StressSurvey"),
        "Time": get_value(model5.params, "Time"),
        "R-squared": model5.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ Stress + Time",
        "Intercept": get_value(model6.params, "Intercept"),
        "Stress/Proxy": get_value(model6.params, "Stress"),
        "Time": get_value(model6.params, "Time"),
        "R-squared": model6.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Subset (≤6): Anxiety ~ StressSurvey + Time",
        "Intercept": get_value(model9.params, "Intercept"),
        "Stress/Proxy": get_value(model9.params, "StressSurvey"),
        "Time": get_value(model9.params, "Time"),
        "R-squared": model9.rsquared,
    }
)

summary_df = pd.DataFrame(summary_rows).round({"Intercept": 3, "Stress/Proxy": 3, "Time": 3, "R-squared": 3})
print(summary_df.to_markdown(index=False))
```

**Coefficient Accuracy vs. Fit:** The proxy-based multiple regression boasts the second-highest $R^2$ yet supplies the most misleading coefficients. Even the subset model with perfect fit reports a StressSurvey slope of 0.333, which is “right” only because the survey scale is exactly three times the stress scale in that limited range. This table is a cheat sheet for beginners: always look at the meaning of the coefficients first, then decide whether the fit statistic is telling a coherent story.

### Question 8: Real-World Implications

**Model 1 (StressSurvey + Time) – Headline:** *“New Regression Finds Every Extra Hour of Social Media Cuts Anxiety by 2.8 Points.”*  
Imagine a newsroom glancing only at the regression output: the Time coefficient is negative, the p-value is tiny, and the $R^2$ is strong. It takes discipline to dig deeper and realize the stress control is a bent survey proxy. Without that context, the story becomes “more scrolling, less anxiety.”

**Model 2 (Stress + Time) – Headline:** *“Even at the Same Stress Level, More Social Media Raises Anxiety.”*  
Here the data use the real stress measure, so the coefficient on Time is the modest +0.10 we expected. The same newsroom would now publish a cautionary tale about screen time harming mental health.

These competing headlines show how measurement choices—not statistical significance—drive public narratives. Parents primed to worry about social media will rally around Model 2; platform executives will amplify Model 1. Both sides can wave peer-reviewed regressions with impressive $R^2$ values, even though only one model reflects the causal truth.

### Question 9: Subset Analysis and Graphical Diagnostics

```{python}
#| label: reg9-subset
print("=" * 70)
print("Question 9: Subset Analysis to Avoid Misleading Significance")
print("=" * 70)

print("\nStrategy: analyze the lower-stress regime (StressSurvey ≤ 6) where the proxy is closest to linear.")
print(f"  Original sample size: {len(observDF)}")
print(f"  Subset sample size: {len(subset_low)}")
print(f"  Observations excluded: {len(observDF) - len(subset_low)}")

print("\nSubset Regression Results:")
print("-" * 70)
print(regression_markdown(model9))
print(f"\nR-squared: {model9.rsquared:.3f}")

print("\nTrue Relationship: Anxiety = Stress + 0.1 × Time")
print("  Target coefficients: Intercept = 0.0, Stress = 1.0, Time = 0.1")

print("\nComparison with full proxy model:")
print("-" * 70)
print(f"  Full-data StressSurvey coefficient: {get_value(model5.params, 'StressSurvey'):.4f}")
print(f"  Subset StressSurvey coefficient: {get_value(model9.params, 'StressSurvey'):.4f}")
print(f"  Full-data Time coefficient: {get_value(model5.params, 'Time'):.4f}")
print(f"  Subset Time coefficient: {get_value(model9.params, 'Time'):.4f}")
print(f"  Full-data R-squared: {model5.rsquared:.4f}")
print(f"  Subset R-squared: {model9.rsquared:.4f}")

print("\nSubset p-values:")
for term, pval in model9.pvalues.items():
    print(f"  {term}: {pval:.4e}")
```

```{python}
#| label: fig-subset-analysis
#| fig-cap: "Subset selection and coefficient comparison"
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

ax1 = axes[0]
ax1.scatter(
    observDF["Stress"],
    observDF["StressSurvey"],
    s=100,
    alpha=0.5,
    color="lightgray",
    label="All data",
)
ax1.scatter(
    subset_low["Stress"],
    subset_low["StressSurvey"],
    s=150,
    alpha=0.8,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
    label="Subset (StressSurvey ≤ 6)",
)
ax1.plot(observDF["Stress"], observDF["StressSurvey"], linewidth=2, color="purple", alpha=0.3)
ax1.set_xlabel("Actual Stress Level", fontsize=11, fontweight="bold")
ax1.set_ylabel("Stress Survey Response", fontsize=11, fontweight="bold")
ax1.set_title("Subset Selection: Lower Stress Regime", fontsize=12, fontweight="bold")
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2 = axes[1]
models = ["Full Model\n(All Data)", "Subset Model\n(≤6)", "True Value"]
stress_coefs = [get_value(model5.params, "StressSurvey"), get_value(model9.params, "StressSurvey"), 1.0]
time_coefs = [get_value(model5.params, "Time"), get_value(model9.params, "Time"), 0.1]
x_pos = np.arange(len(models))
width = 0.35
bars1 = ax2.bar(
    x_pos - width / 2,
    stress_coefs,
    width,
    label="Stress/Proxy",
    color="steelblue",
    alpha=0.7,
    edgecolor="black",
    linewidth=1.5,
)
bars2 = ax2.bar(
    x_pos + width / 2,
    time_coefs,
    width,
    label="Time",
    color="coral",
    alpha=0.7,
    edgecolor="black",
    linewidth=1.5,
)
ax2.axhline(y=1.0, color="green", linestyle="--", linewidth=1.5, alpha=0.5, label="True Stress = 1.0")
ax2.axhline(y=0.1, color="green", linestyle=":", linewidth=1.5, alpha=0.5, label="True Time = 0.1")
ax2.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax2.set_title("Coefficient Comparison: Full vs Subset", fontsize=12, fontweight="bold")
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)
ax2.legend()
ax2.grid(True, alpha=0.3, axis="y")
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.3f}",
            ha="center",
            va="bottom",
            fontsize=9,
            fontweight="bold",
        )

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-graphical-diagnostics
#| fig-cap: "Residual diagnostics: full proxy model vs subset model"
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

ax1 = axes[0, 0]
fitted_full = model5.fittedvalues
residuals_full = model5.resid
ax1.scatter(fitted_full, residuals_full, s=100, alpha=0.7, color="steelblue", edgecolors="black", linewidth=1)
ax1.axhline(y=0, color="red", linestyle="--", linewidth=2)
ax1.set_xlabel("Fitted Values", fontsize=11, fontweight="bold")
ax1.set_ylabel("Residuals", fontsize=11, fontweight="bold")
ax1.set_title("Full Proxy Model: Residuals vs Fitted", fontsize=12, fontweight="bold")
ax1.grid(True, alpha=0.3)

ax2 = axes[0, 1]
fitted_subset = model9.fittedvalues
residuals_subset = model9.resid
ax2.scatter(fitted_subset, residuals_subset, s=100, alpha=0.7, color="coral", edgecolors="black", linewidth=1)
ax2.axhline(y=0, color="red", linestyle="--", linewidth=2)
ax2.set_xlabel("Fitted Values", fontsize=11, fontweight="bold")
ax2.set_ylabel("Residuals", fontsize=11, fontweight="bold")
ax2.set_title("Subset Model: Residuals vs Fitted", fontsize=12, fontweight="bold")
ax2.grid(True, alpha=0.3)

ax3 = axes[1, 0]
ax3.scatter(
    observDF["StressSurvey"],
    observDF["Anxiety"],
    s=80,
    alpha=0.4,
    color="lightgray",
    label="All data",
)
ax3.scatter(
    subset_low["StressSurvey"],
    subset_low["Anxiety"],
    s=120,
    alpha=0.8,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
    label="Subset data",
)
x_line_full = np.linspace(observDF["StressSurvey"].min(), observDF["StressSurvey"].max(), 100)
x_line_full_df = pd.DataFrame(
    {
        "const": np.ones_like(x_line_full),
        "StressSurvey": x_line_full,
        "Time": np.full_like(x_line_full, observDF["Time"].mean()),
    }
)
y_line_full = model5.predict(x_line_full_df)
ax3.plot(x_line_full, y_line_full, "r--", linewidth=2, alpha=0.7, label=f"Full model (R²={model5.rsquared:.3f})")
x_line_subset = np.linspace(subset_low["StressSurvey"].min(), subset_low["StressSurvey"].max(), 100)
x_line_subset_df = pd.DataFrame(
    {
        "const": np.ones_like(x_line_subset),
        "StressSurvey": x_line_subset,
        "Time": np.full_like(x_line_subset, subset_low["Time"].mean()),
    }
)
y_line_subset = model9.predict(x_line_subset_df)
ax3.plot(x_line_subset, y_line_subset, "g-", linewidth=2, label=f"Subset model (R²={model9.rsquared:.3f})")
ax3.set_xlabel("Stress Survey Response", fontsize=11, fontweight="bold")
ax3.set_ylabel("Anxiety Level", fontsize=11, fontweight="bold")
ax3.set_title("Full vs Subset Fit", fontsize=12, fontweight="bold")
ax3.legend()
ax3.grid(True, alpha=0.3)

ax4 = axes[1, 1]
stats.probplot(residuals_subset, dist="norm", plot=ax4)
ax4.set_title("Subset Model: Q-Q Plot", fontsize=12, fontweight="bold")
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

Focusing on `StressSurvey ≤ 6` yields

$$\widehat{Anxiety} = 0.000 + 0.333 \times StressSurvey + 0.100 \times Time,$$

with $R^2 = 1.000$. In this calmer stress regime, the survey happens to track cortisol in a neat three-to-one ratio, so the regression adjusts the slope to 0.333 (one-third of the true coefficient) and recovers the correct +0.100 for Time. The residual panels translate the statistics into pictures: the full model’s residuals bend upward (a sign of non-linearity), while the subset residuals sit quietly around zero. The Q-Q plot likewise shows the subset residuals behaving normally. For beginners, this is the recipe: slice the data where your proxy behaves almost linearly, check the residuals, and only then trust the regression story.

**Takeaway:** Segmenting the sample by stress severity and examining residual patterns surfaces hidden non-linearities. Rather than trusting the full-sample regression blindly, this approach pinpoints when measurement error will upend causal interpretation.

