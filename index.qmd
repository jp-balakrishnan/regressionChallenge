---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Interrogating Linear Regression with Non-Linear Proxies

This analysis examines the implications of using monotonic proxy variables as substitutes for true causal variables in linear regression models. Through a controlled stress‚Äìanxiety scenario, we demonstrate how seemingly reasonable proxy variables can systematically bias regression estimates despite maintaining strong statistical fit. The following sections present regression analyses, visualizations, and interpretations that address each component of the challenge requirements.

$$
\begin{aligned}
A &\equiv \textrm{Anxiety Level measured by fMRI activity}\\
S &\equiv \textrm{Stress Level measured by cortisol level in blood}\\
T &\equiv \textrm{\# of minutes on social media in last 24 hours}
\end{aligned}
$$

Let's assume we **know** the relationship among these variables is as follows:

$$\widehat{Anxiety} = StressSurvey + 0.100 \times Time,$$

::: {.callout-important}
## üîç Understanding the True Relationship: Implied Coefficients

**Critical Point:** Students often miss that this specific equation implies specific coefficient values in the generic multiple regression framework.

**The Generic Multiple Regression Equation:**
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

**In Our Case:**
$$
Anxiety = \beta_0 + \beta_1 \times Stress + \beta_2 \times Time + \epsilon
$$

**The True Coefficients (what we "know"):**

- $\beta_0 = 0$ (intercept is zero)
- $\beta_1 = 1$ (coefficient on Stress is 1)  
- $\beta_2 = 0.1$ (coefficient on Time is 0.1)

:::

## Data and Proxy Diagnostics

```{python}
#| label: setup
#| include: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy import stats

observDF = pd.DataFrame(
    {
        "Stress": [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
        "StressSurvey": [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
        "Time": [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
        "Anxiety": [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22],
    }
)

sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (8, 5)


def fit_ols(features, data):
    X = sm.add_constant(data[features])
    y = data["Anxiety"]
    return sm.OLS(y, X).fit()


def regression_markdown(model):
    params = model.params.rename({"const": "Intercept"})
    conf_int = model.conf_int().rename(index={"const": "Intercept"})
    stats_df = pd.DataFrame(
        {
            "Estimate": params.round(3),
            "Std. Error": model.bse.round(3),
            "t": model.tvalues.round(2),
            "p-value": model.pvalues.apply(lambda x: f"{x:.3g}"),
            "95% CI Low": conf_int[0].round(3),
            "95% CI High": conf_int[1].round(3),
        }
    )
    return stats_df.to_markdown()


def get_value(series, name):
    if name in series.index:
        return series[name]
    if name == "Intercept" and "const" in series.index:
        return series["const"]
    raise KeyError(f"{name} not found in index {list(series.index)}")


model_results = {
    "q1_stresssurvey": fit_ols(["StressSurvey"], observDF),
    "q3_time": fit_ols(["Time"], observDF),
    "q5_proxy_multi": fit_ols(["StressSurvey", "Time"], observDF),
    "q6_true_multi": fit_ols(["Stress", "Time"], observDF),
}

subset_low = observDF[observDF["StressSurvey"] <= 6].copy()
model_results["q9_subset"] = fit_ols(["StressSurvey", "Time"], subset_low)

model1 = model_results["q1_stresssurvey"]
model3 = model_results["q3_time"]
model5 = model_results["q5_proxy_multi"]
model6 = model_results["q6_true_multi"]
model9 = model_results["q9_subset"]
```

```{python}
#| label: tbl-observations
#| tbl-cap: "Observed data with known true relationships"
observDF
```

```{python}
#| label: fig-stress-proxy
#| fig-cap: "StressSurvey is monotonic but not linear in true stress"
fig, ax = plt.subplots()
ax.plot(
    observDF["Stress"],
    observDF["StressSurvey"],
    linewidth=1,
    color="purple",
    marker="o",
    markersize=12,
)
ax.set_title("Survey Proxy vs. Actual Stress")
ax.set_xlabel("Actual Stress Level")
ax.set_ylabel("Stress Survey Response")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## Analysis and Answers

### Question 1: Bivariate Regression with StressSurvey

Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?
```{python}
#| label: reg1-stresssurvey
print("=" * 70)
print("Question 1: Bivariate Regression - Anxiety ~ StressSurvey")
print("=" * 70)
print(regression_markdown(model1))
print(f"\nR-squared: {model1.rsquared:.3f}")
```

The estimated regression equation is:

$$\widehat{Anxiety} = -1.524 + 1.047 \times StressSurvey.$$

This model indicates that anxiety begins at ‚àí1.524 units when the survey reports zero stress, and increases by approximately 1.047 anxiety units for each one-unit increase in the survey response. The model demonstrates strong statistical fit with an $R^2$ of 0.901, indicating that the linear regression line closely approximates the observed data. However, this apparent success masks a fundamental problem: the survey instrument serves as a proxy for the true cortisol-based stress measurement, and the relationship between the proxy and true stress is non-linear, accelerating more rapidly at higher stress levels. To accommodate this non-linearity while maintaining linearity assumptions, the regression model adjusts the slope coefficient above the true value of 1.0 and shifts the intercept below zero. Consequently, while the model exhibits strong fit statistics, the estimated coefficients systematically deviate from the true causal parameters.

### Question 2: Visualization of Bivariate Relationship ‚Äì StressSurvey

Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.
```{python}
#| label: fig-stresssurvey-anxiety
#| fig-cap: "StressSurvey vs Anxiety with fitted line"
fig, ax = plt.subplots()
ax.scatter(
    observDF["StressSurvey"],
    observDF["Anxiety"],
    s=100,
    alpha=0.7,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
)
x_line = np.linspace(observDF["StressSurvey"].min(), observDF["StressSurvey"].max(), 100)
x_line_df = pd.DataFrame({"StressSurvey": x_line})
y_line = model1.predict(sm.add_constant(x_line_df))
ax.plot(x_line, y_line, "r-", linewidth=2, label=f"Regression line (R¬≤ = {model1.rsquared:.3f})")
ax.set_xlabel("Stress Survey Response", fontsize=12, fontweight="bold")
ax.set_ylabel("Anxiety Level", fontsize=12, fontweight="bold")
ax.set_title("Bivariate Relationship: StressSurvey vs Anxiety", fontsize=14, fontweight="bold")
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

The scatter plot displays Anxiety as a function of StressSurvey responses. 
* The data exhibit a stepped pattern: for each true stress level, the survey instrument records three distinct anxiety observations. 
* The transition from survey score 9 to 12 demonstrates a more pronounced increase compared to earlier transitions, indicating **non-linear** scaling between the proxy measure and true stress. 
* The fitted regression line averages across these stepped observations, resulting in systematic over-prediction for low-stress observations and under-prediction for high-stress observations. 
* The high R-squared gives false confidence. The model appears perfect, but it‚Äôs estimating the wrong relationship because it‚Äôs missing a key variable.
* This visualization demonstrates the geometric manifestation of non-linearity: although the proxy variable maintains a monotonic relationship with true stress, the deviation from linearity systematically biases the estimated slope coefficient, even when this bias is not immediately apparent from summary statistics.

### Question 3: Bivariate Regression with Time

Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?
```{python}
#| label: reg3-time
print("=" * 70)
print("Question 3: Bivariate Regression - Anxiety ~ Time")
print("=" * 70)
print(regression_markdown(model3))
print(f"\nR-squared: {model3.rsquared:.3f}")
```

Regressing Anxiety on Time produces the following model:

$$\widehat{Anxiety} = -3.680 + 5.341 \times Time,$$

with $R^2 = 0.563$. Under this specification, the intercept suggests ‚àí3.68 anxiety units at zero social media use, which is conceptually implausible, and the slope coefficient of 5.341 indicates that each additional hour of social media use is associated with a 5.34-unit increase in anxiety. 
* This estimate substantially exceeds the true marginal effect of 0.1 units per hour that was specified in the data-generating process. 
* The discrepancy arises from **omitted-variable bias**: because Stress is excluded from this bivariate regression, the model attributes all stress-induced variation in anxiety to the Time variable. 
* This results in a coefficient estimate that is approximately **53 times larger** than the true causal effect, demonstrating how confounding variables can dramatically distort regression coefficients even when the omitted variable is conceptually distinct from the included regressor.

### Question 4: Visualization of Bivariate Relationship ‚Äì Time

Create a scatter plot with the regression line showing the relationship between Time and Anxiety. Comment on the fit and any potential issues.
```{python}
#| label: fig-time-anxiety
#| fig-cap: "Time vs Anxiety with fitted line"
fig, ax = plt.subplots()
ax.scatter(
    observDF["Time"],
    observDF["Anxiety"],
    s=100,
    alpha=0.7,
    color="coral",
    edgecolors="black",
    linewidth=1.5,
)
x_line = np.linspace(observDF["Time"].min(), observDF["Time"].max(), 100)
x_line_df = pd.DataFrame({"Time": x_line})
y_line = model3.predict(sm.add_constant(x_line_df))
ax.plot(x_line, y_line, "r-", linewidth=2, label=f"Regression line (R¬≤ = {model3.rsquared:.3f})")
ax.set_xlabel("Time on Social Media (hours)", fontsize=12, fontweight="bold")
ax.set_ylabel("Anxiety Level", fontsize=12, fontweight="bold")
ax.set_title("Bivariate Relationship: Time vs Anxiety", fontsize=14, fontweight="bold")
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

The scatter plot displays Time on social media versus Anxiety. 

* The data points form vertical clusters rather than a consistent upward trajectory, indicating substantial variation in anxiety levels even when social media use remains constant. 
* This vertical dispersion reflects the influence of unobserved heterogeneity‚Äîspecifically, differences in true stress levels among individuals with similar social media use.
* The regression model, which does not account for stress variation, fits a steep positive slope through these clusters, attributing all observed variation in anxiety to social media use. 
* This visualization illustrates why the Time coefficient is inflated: the model attempts to explain vertical variation (driven by stress differences) using only horizontal variation (time spent on social media), leading to systematic overestimation of the social media effect.

### Question 5: Multiple Regression with StressSurvey and Time

Run a multiple regression of Anxiety on both **StressSurvey** and Time. What are the estimated coefficients? How do they compare to the true relationship?
```{python}
#| label: reg5-multiple-stresssurvey
print("=" * 70)
print("Question 5: Multiple Regression - Anxiety ~ StressSurvey + Time")
print("=" * 70)
print(regression_markdown(model5))
print(f"\nR-squared: {model5.rsquared:.3f}")
```

The multiple regression model including both StressSurvey and Time yields:

$$\widehat{Anxiety} = 0.589 + 1.427 \times StressSurvey - 2.780 \times Time,$$

with $R^2 = 0.935$. A naive interpretation would suggest that each additional unit on the stress survey scale is associated with a 1.427-unit increase in anxiety, while each additional hour of social media use is associated with a 2.780-unit *decrease* in anxiety. 

* Both coefficients are statistically significant, yet both estimates are substantially biased. 
* The **non-linear** relationship between the StressSurvey proxy and true stress causes the regression model to compensate by inflating the StressSurvey coefficient and reversing the sign of the Time coefficient. 
* This demonstrates a critical problem: a regression model can exhibit **strong statistical fit** (high $R^2$) and statistically significant coefficients while producing estimates that are **qualitatively incorrect**. 
* The model provides a confident but **misleading** representation of the true causal relationships.

### Question 6: Multiple Regression with True Stress and Time

Run a multiple regression of Anxiety on both **Stress** and Time. What are the estimated coefficients? How do they compare to the true relationship?
```{python}
#| label: reg6-multiple-stress
print("=" * 70)
print("Question 6: Multiple Regression - Anxiety ~ Stress + Time")
print("=" * 70)
print(regression_markdown(model6))
print(f"\nR-squared: {model6.rsquared:.3f}")
```

When the true Stress variable is substituted for the StressSurvey proxy, the regression model perfectly recovers the data-generating process:

$$\widehat{Anxiety} = 0.000 + 1.000 \times Stress + 0.100 \times Time,$$

with $R^2 = 1.000$. Perfect $R^2$ means the line passes through every observed point, so there is **no residual variation**.

* This specification yields an intercept of exactly zero, indicating **no anxiety** when both stress and social media use are zero, which aligns with the data-generating process. 
* The Stress coefficient is exactly 1.000, indicating a **one-to-one relationship** between stress units and anxiety units. 
* The Time coefficient is exactly 0.100, indicating that each additional hour of social media use increases anxiety by **0.1 units**. 
* A perfect $R^2$ of 1.000 indicates that the regression line passes through all observed data points, leaving **no residual variation**. 
* The only change between this model and the previous specification is the substitution of the **true stress measurement** for the proxy variable. 
* This single modification demonstrates that the quality of variable measurement is fundamental to regression validity: even with identical sample size, model specification, and estimation technique, the use of a linear proxy variable produces substantially biased estimates, while the true variable yields **accurate parameter estimates**.

### Question 7: Model Comparison

Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?
```{python}
#| label: reg7-comparison
print("=" * 70)
print("Question 7: Model Comparison")
print("=" * 70)

print("\nModel 1: Anxiety ~ StressSurvey + Time")
print("-" * 70)
print(f"  R-squared: {model5.rsquared:.4f}")
print(f"  Intercept: {get_value(model5.params, 'Intercept'):.4f}")
print(f"  StressSurvey coefficient: {get_value(model5.params, 'StressSurvey'):.4f}")
print(f"  Time coefficient: {get_value(model5.params, 'Time'):.4f}")
print(f"\n  Statistical Significance:")
pvals5 = model5.pvalues
print(f"    Intercept p-value: {get_value(pvals5, 'Intercept'):.4e}")
print(f"    StressSurvey p-value: {get_value(pvals5, 'StressSurvey'):.4e}")
print(f"    Time p-value: {get_value(pvals5, 'Time'):.4e}")

print("\nModel 2: Anxiety ~ Stress + Time")
print("-" * 70)
print(f"  R-squared: {model6.rsquared:.4f}")
print(f"  Intercept: {get_value(model6.params, 'Intercept'):.4f}")
print(f"  Stress coefficient: {get_value(model6.params, 'Stress'):.4f}")
print(f"  Time coefficient: {get_value(model6.params, 'Time'):.4f}")
print(f"\n  Statistical Significance:")
pvals6 = model6.pvalues
print(f"    Intercept p-value: {get_value(pvals6, 'Intercept'):.4e}")
print(f"    Stress p-value: {get_value(pvals6, 'Stress'):.4e}")
print(f"    Time p-value: {get_value(pvals6, 'Time'):.4e}")

print("\n" + "=" * 70)
print("Key Comparison Points:")
print("=" * 70)

stresssurvey_pct_error = (get_value(model5.params, "StressSurvey") - 1.0) / 1.0 * 100
stress_pct_error = (get_value(model6.params, "Stress") - 1.0) / 1.0 * 100
time_proxy_pct_error = (get_value(model5.params, "Time") - 0.1) / 0.1 * 100
time_true_pct_error = (get_value(model6.params, "Time") - 0.1) / 0.1 * 100

print(f"\n1. R-squared:")
print(f"   Proxy model: {model5.rsquared:.4f}")
print(f"   True-stress model: {model6.rsquared:.4f}")
print(f"   Absolute difference: {abs(model5.rsquared - model6.rsquared):.4f}")

print(f"\n2. Stress-related coefficient:")
print(f"   Proxy coefficient: {get_value(model5.params, 'StressSurvey'):.4f} ({stresssurvey_pct_error:.1f}% off)")
print(f"   True coefficient: {get_value(model6.params, 'Stress'):.4f} ({stress_pct_error:.1f}% off)")

print(f"\n3. Time coefficient:")
print(f"   Proxy model: {get_value(model5.params, 'Time'):.4f} ({time_proxy_pct_error:.1f}% off)")
print(f"   True-stress model: {get_value(model6.params, 'Time'):.4f} ({time_true_pct_error:.1f}% off)")

print("\n4. Statistical significance:")
print("   Both models clear the 5% threshold on every coefficient, despite only one set matching reality.")
```

```{python}
#| label: fig-comparison
#| fig-cap: "Coefficient comparison between proxy-based and true-stress models"
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax1 = axes[0]
coefficients = ["StressSurvey\n(Proxy Model)", "Stress\n(True Model)", "True Value"]
values = [get_value(model5.params, "StressSurvey"), get_value(model6.params, "Stress"), 1.0]
colors = ["steelblue", "coral", "green"]
bars = ax1.bar(coefficients, values, color=colors, alpha=0.7, edgecolor="black", linewidth=1.5)
ax1.axhline(y=1.0, color="green", linestyle="--", linewidth=2, label="True Value (1.0)")
ax1.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax1.set_title("Stress Coefficient Comparison", fontsize=12, fontweight="bold")
ax1.legend()
ax1.grid(True, alpha=0.3, axis="y")
for bar, val in zip(bars, values):
    ax1.text(bar.get_x() + bar.get_width() / 2.0, val, f"{val:.3f}", ha="center", va="bottom", fontweight="bold")

ax2 = axes[1]
coefficients_time = ["Proxy Model", "True Model", "True Value"]
values_time = [get_value(model5.params, "Time"), get_value(model6.params, "Time"), 0.1]
bars2 = ax2.bar(coefficients_time, values_time, color=colors, alpha=0.7, edgecolor="black", linewidth=1.5)
ax2.axhline(y=0.1, color="green", linestyle="--", linewidth=2, label="True Value (0.1)")
ax2.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax2.set_title("Time Coefficient Comparison", fontsize=12, fontweight="bold")
ax2.legend()
ax2.grid(True, alpha=0.3, axis="y")
for bar, val in zip(bars2, values_time):
    ax2.text(bar.get_x() + bar.get_width() / 2.0, val, f"{val:.3f}", ha="center", va="bottom", fontweight="bold")

plt.tight_layout()
plt.show()
```

The comparison between models reveals both similarities and critical differences. Each model produces a goodness-of-fit measure ($R^2$) and individual coefficient estimates for Stress (or its proxy) and Time. The bar charts provide a visual representation of these comparisons: coefficient estimates that align closely with the green dashed reference lines (representing true parameter values) indicate accurate estimation, while estimates that deviate substantially from these reference values demonstrate the magnitude of bias.

*R-squared:* The coefficient of determination ($R^2$) measures the proportion of variance in the dependent variable that the model explains. The proxy-based model achieves an $R^2$ of 0.935, which is nearly indistinguishable from the perfect fit ($R^2 = 1.000$) obtained by the true-stress model. If model selection were based solely on goodness-of-fit measures, both models would appear equally valid.

*Coefficients:* The substantive interpretation of the models diverges dramatically at the coefficient level. The proxy model estimates that each additional unit on the stress survey scale is associated with a 1.427-unit increase in anxiety (representing a 43% overestimate relative to the true coefficient of 1.0), while each additional hour of social media use is associated with a 2.780-unit *decrease* in anxiety (a substantial departure from the true positive effect of +0.10). In contrast, the true-stress model produces coefficient estimates of exactly 1.000 and +0.100, perfectly matching the data-generating process. This comparison demonstrates that even small discrepancies in coefficient estimates can lead to fundamentally different substantive conclusions and policy implications.

*Statistical Significance:* Both models report extremely small p-values for all coefficients, indicating that the null hypothesis of zero effect can be rejected at conventional significance levels. However, statistical significance only indicates that an estimated coefficient is unlikely to equal zero *if the model assumptions hold*. It does not guarantee that the estimated coefficient represents the true causal effect, nor does it validate the model specification or the quality of the included variables. This distinction is crucial: researchers must evaluate coefficient estimates not only for statistical significance but also for substantive validity, which requires careful consideration of measurement quality, model specification, and the theoretical relationship between variables.

```{python}
#| label: tbl-model-summary
summary_rows = []
summary_rows.append(
    {
        "Model": "Anxiety ~ StressSurvey",
        "Intercept": get_value(model1.params, "Intercept"),
        "Stress/Proxy": get_value(model1.params, "StressSurvey"),
        "Time": "",
        "R-squared": model1.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ Time",
        "Intercept": get_value(model3.params, "Intercept"),
        "Stress/Proxy": "",
        "Time": get_value(model3.params, "Time"),
        "R-squared": model3.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ StressSurvey + Time",
        "Intercept": get_value(model5.params, "Intercept"),
        "Stress/Proxy": get_value(model5.params, "StressSurvey"),
        "Time": get_value(model5.params, "Time"),
        "R-squared": model5.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Anxiety ~ Stress + Time",
        "Intercept": get_value(model6.params, "Intercept"),
        "Stress/Proxy": get_value(model6.params, "Stress"),
        "Time": get_value(model6.params, "Time"),
        "R-squared": model6.rsquared,
    }
)
summary_rows.append(
    {
        "Model": "Subset (‚â§6): Anxiety ~ StressSurvey + Time",
        "Intercept": get_value(model9.params, "Intercept"),
        "Stress/Proxy": get_value(model9.params, "StressSurvey"),
        "Time": get_value(model9.params, "Time"),
        "R-squared": model9.rsquared,
    }
)

summary_df = pd.DataFrame(summary_rows).round({"Intercept": 3, "Stress/Proxy": 3, "Time": 3, "R-squared": 3})
print(summary_df.to_markdown(index=False))
```

**Coefficient Accuracy vs. Fit:** This summary table demonstrates a critical distinction between statistical fit and parameter accuracy. The proxy-based multiple regression achieves the second-highest $R^2$ among all models (0.935) yet produces the most substantially biased coefficient estimates. Even the subset model, which achieves perfect fit ($R^2 = 1.000$), reports a StressSurvey coefficient of 0.333. This value appears consistent with the true relationship only because, within the restricted range of the subset (StressSurvey ‚â§ 6), the survey scale maintains an exact three-to-one ratio with the true stress scale. This observation illustrates an important principle for regression analysis: researchers should prioritize the substantive interpretation of coefficient estimates over goodness-of-fit statistics. A model with strong fit but biased coefficients may provide misleading substantive conclusions, whereas evaluating coefficients in the context of theoretical expectations and measurement quality is essential for valid inference.

### Question 8: Real-World Implications
For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press.  What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model?  Assuming confirmation bias is real, which model is a typical parent going to believe?  Which model will Facebook, Instagram, and TikTok executives prefer?

**Model 1 (StressSurvey + Time) ‚Äì Headline:** *"New Regression Finds Every Extra Hour of Social Media Cuts Anxiety by 2.8 Points."*  
A media outlet reporting on the proxy-based regression would likely focus on the negative Time coefficient (-2.78), the extremely small p-value, and the strong $R^2$ (0.935), interpreting these as evidence that social media use reduces anxiety when stress is "controlled for." However, this interpretation overlooks a critical methodological detail: the stress control variable is a proxy measure with a non-linear relationship to true stress. Without understanding this measurement limitation, the reporting would suggest that increased social media use is associated with reduced anxiety, which contradicts the true causal relationship.

**Model 2 (Stress + Time) ‚Äì Headline:** *"Even at the Same Stress Level, More Social Media Raises Anxiety."*  
When the true stress measurement is used, the Time coefficient becomes the expected positive value of +0.10, indicating that social media use is associated with increased anxiety even after controlling for stress levels. The same media outlet would now report findings consistent with concerns about the potential negative effects of social media on mental health.

These contrasting headlines demonstrate how measurement choices, rather than statistical significance tests, fundamentally shape public interpretation of research findings. Stakeholders with pre-existing concerns about social media (e.g., parents, mental health advocates) would likely emphasize findings from Model 2, while stakeholders with incentives to minimize concerns about social media (e.g., platform executives) would likely highlight findings from Model 1. Both models can be presented as valid, peer-reviewed research with strong statistical fit, despite the fact that only one model accurately reflects the true causal relationships. This scenario illustrates how measurement quality affects not only scientific validity but also public understanding and policy debates.

### Question 9: Subset Analysis and Graphical Diagnostics
Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why?  Did you get results that are both statistically significant and close to the true relationship?

```{python}
#| label: reg9-subset
print("=" * 70)
print("Question 9: Subset Analysis to Avoid Misleading Significance")
print("=" * 70)

print("\nStrategy: analyze the lower-stress regime (StressSurvey ‚â§ 6) where the proxy is closest to linear.")
print(f"  Original sample size: {len(observDF)}")
print(f"  Subset sample size: {len(subset_low)}")
print(f"  Observations excluded: {len(observDF) - len(subset_low)}")

print("\nSubset Regression Results:")
print("-" * 70)
print(regression_markdown(model9))
print(f"\nR-squared: {model9.rsquared:.3f}")

print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("  Target coefficients: Intercept = 0.0, Stress = 1.0, Time = 0.1")

print("\nComparison with full proxy model:")
print("-" * 70)
print(f"  Full-data StressSurvey coefficient: {get_value(model5.params, 'StressSurvey'):.4f}")
print(f"  Subset StressSurvey coefficient: {get_value(model9.params, 'StressSurvey'):.4f}")
print(f"  Full-data Time coefficient: {get_value(model5.params, 'Time'):.4f}")
print(f"  Subset Time coefficient: {get_value(model9.params, 'Time'):.4f}")
print(f"  Full-data R-squared: {model5.rsquared:.4f}")
print(f"  Subset R-squared: {model9.rsquared:.4f}")

print("\nSubset p-values:")
for term, pval in model9.pvalues.items():
    print(f"  {term}: {pval:.4e}")
```

```{python}
#| label: fig-subset-analysis
#| fig-cap: "Subset selection and coefficient comparison"
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

ax1 = axes[0]
ax1.scatter(
    observDF["Stress"],
    observDF["StressSurvey"],
    s=100,
    alpha=0.5,
    color="lightgray",
    label="All data",
)
ax1.scatter(
    subset_low["Stress"],
    subset_low["StressSurvey"],
    s=150,
    alpha=0.8,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
    label="Subset (StressSurvey ‚â§ 6)",
)
ax1.plot(observDF["Stress"], observDF["StressSurvey"], linewidth=2, color="purple", alpha=0.3)
ax1.set_xlabel("Actual Stress Level", fontsize=11, fontweight="bold")
ax1.set_ylabel("Stress Survey Response", fontsize=11, fontweight="bold")
ax1.set_title("Subset Selection: Lower Stress Regime", fontsize=12, fontweight="bold")
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2 = axes[1]
models = ["Full Model\n(All Data)", "Subset Model\n(‚â§6)", "True Value"]
stress_coefs = [get_value(model5.params, "StressSurvey"), get_value(model9.params, "StressSurvey"), 1.0]
time_coefs = [get_value(model5.params, "Time"), get_value(model9.params, "Time"), 0.1]
x_pos = np.arange(len(models))
width = 0.35
bars1 = ax2.bar(
    x_pos - width / 2,
    stress_coefs,
    width,
    label="Stress/Proxy",
    color="steelblue",
    alpha=0.7,
    edgecolor="black",
    linewidth=1.5,
)
bars2 = ax2.bar(
    x_pos + width / 2,
    time_coefs,
    width,
    label="Time",
    color="coral",
    alpha=0.7,
    edgecolor="black",
    linewidth=1.5,
)
ax2.axhline(y=1.0, color="green", linestyle="--", linewidth=1.5, alpha=0.5, label="True Stress = 1.0")
ax2.axhline(y=0.1, color="green", linestyle=":", linewidth=1.5, alpha=0.5, label="True Time = 0.1")
ax2.set_ylabel("Coefficient Value", fontsize=11, fontweight="bold")
ax2.set_title("Coefficient Comparison: Full vs Subset", fontsize=12, fontweight="bold")
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)
ax2.legend()
ax2.grid(True, alpha=0.3, axis="y")
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.3f}",
            ha="center",
            va="bottom",
            fontsize=9,
            fontweight="bold",
        )

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-graphical-diagnostics
#| fig-cap: "Residual diagnostics: full proxy model vs subset model"
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

ax1 = axes[0, 0]
fitted_full = model5.fittedvalues
residuals_full = model5.resid
ax1.scatter(fitted_full, residuals_full, s=100, alpha=0.7, color="steelblue", edgecolors="black", linewidth=1)
ax1.axhline(y=0, color="red", linestyle="--", linewidth=2)
ax1.set_xlabel("Fitted Values", fontsize=11, fontweight="bold")
ax1.set_ylabel("Residuals", fontsize=11, fontweight="bold")
ax1.set_title("Full Proxy Model: Residuals vs Fitted", fontsize=12, fontweight="bold")
ax1.grid(True, alpha=0.3)

ax2 = axes[0, 1]
fitted_subset = model9.fittedvalues
residuals_subset = model9.resid
ax2.scatter(fitted_subset, residuals_subset, s=100, alpha=0.7, color="coral", edgecolors="black", linewidth=1)
ax2.axhline(y=0, color="red", linestyle="--", linewidth=2)
ax2.set_xlabel("Fitted Values", fontsize=11, fontweight="bold")
ax2.set_ylabel("Residuals", fontsize=11, fontweight="bold")
ax2.set_title("Subset Model: Residuals vs Fitted", fontsize=12, fontweight="bold")
ax2.grid(True, alpha=0.3)

ax3 = axes[1, 0]
ax3.scatter(
    observDF["StressSurvey"],
    observDF["Anxiety"],
    s=80,
    alpha=0.4,
    color="lightgray",
    label="All data",
)
ax3.scatter(
    subset_low["StressSurvey"],
    subset_low["Anxiety"],
    s=120,
    alpha=0.8,
    color="steelblue",
    edgecolors="black",
    linewidth=1.5,
    label="Subset data",
)
x_line_full = np.linspace(observDF["StressSurvey"].min(), observDF["StressSurvey"].max(), 100)
x_line_full_df = pd.DataFrame(
    {
        "const": np.ones_like(x_line_full),
        "StressSurvey": x_line_full,
        "Time": np.full_like(x_line_full, observDF["Time"].mean()),
    }
)
y_line_full = model5.predict(x_line_full_df)
ax3.plot(x_line_full, y_line_full, "r--", linewidth=2, alpha=0.7, label=f"Full model (R¬≤={model5.rsquared:.3f})")
x_line_subset = np.linspace(subset_low["StressSurvey"].min(), subset_low["StressSurvey"].max(), 100)
x_line_subset_df = pd.DataFrame(
    {
        "const": np.ones_like(x_line_subset),
        "StressSurvey": x_line_subset,
        "Time": np.full_like(x_line_subset, subset_low["Time"].mean()),
    }
)
y_line_subset = model9.predict(x_line_subset_df)
ax3.plot(x_line_subset, y_line_subset, "g-", linewidth=2, label=f"Subset model (R¬≤={model9.rsquared:.3f})")
ax3.set_xlabel("Stress Survey Response", fontsize=11, fontweight="bold")
ax3.set_ylabel("Anxiety Level", fontsize=11, fontweight="bold")
ax3.set_title("Full vs Subset Fit", fontsize=12, fontweight="bold")
ax3.legend()
ax3.grid(True, alpha=0.3)

ax4 = axes[1, 1]
stats.probplot(residuals_subset, dist="norm", plot=ax4)
ax4.set_title("Subset Model: Q-Q Plot", fontsize=12, fontweight="bold")
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

Restricting the analysis to observations where `StressSurvey ‚â§ 6` produces the following model:

$$\widehat{Anxiety} = 0.000 + 0.333 \times StressSurvey + 0.100 \times Time,$$

with $R^2 = 1.000$. Within this lower-stress regime, the survey proxy maintains a consistent three-to-one ratio with true stress levels. The regression model adjusts the StressSurvey coefficient to 0.333, which is exactly one-third of the true coefficient value (1.0), reflecting the scaling difference between the proxy and true variable. Importantly, the Time coefficient is correctly estimated as +0.100, matching the true marginal effect. The diagnostic plots illustrate the differences between the full and subset models: the full model's residuals exhibit a systematic curved pattern, indicating violation of the linearity assumption, whereas the subset model's residuals cluster around zero with no apparent pattern. The Q-Q plot confirms that the subset model's residuals approximate a normal distribution, supporting the validity of the linear specification within this restricted range. This analysis demonstrates a methodological approach for dealing with non-linear proxies: partition the data into subsets where the proxy variable maintains approximately linear relationships with the true variable, conduct separate regressions within each subset, and evaluate model assumptions through residual diagnostics before drawing substantive conclusions.

**Takeaway:** This subset analysis demonstrates that stratifying the sample by relevant characteristics (in this case, stress severity) and examining residual patterns can reveal violations of regression assumptions that are obscured in the full sample. Rather than applying regression models to the entire dataset without validation, researchers should evaluate whether relationships are consistent across different data regimes and assess whether measurement limitations vary across the range of observed values. This approach helps identify when proxy variables can be reliably used for causal inference and when they introduce systematic bias that undermines substantive interpretation.

